{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf5xrCdYpLvD"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tarfile\n",
        "import io\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "class NetworkAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Khởi tạo class NetworkAnalyzer\"\"\"\n",
        "        self.data = None\n",
        "        self.graph = None\n",
        "        self.metrics = {}\n",
        "\n",
        "    def download_and_read_data(self, url=None):\n",
        "        \"\"\"\n",
        "        Tải và đọc dữ liệu từ KONECT\n",
        "        Args:\n",
        "            url: URL của dataset (tùy chọn)\n",
        "        Returns:\n",
        "            DataFrame: Chứa thông tin về các cạnh\n",
        "        \"\"\"\n",
        "        if url is None:\n",
        "            url = \"http://konect.cc/files/download.tsv.dblp_coauthor.tar.bz2\"\n",
        "\n",
        "        print(\"Đang tải và đọc dữ liệu...\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                tar = tarfile.open(fileobj=io.BytesIO(response.content), mode='r:bz2')\n",
        "                data_file = [f for f in tar.getmembers() if f.name.endswith('out.dblp_coauthor')][0]\n",
        "                data_content = tar.extractfile(data_file).read().decode('utf-8')\n",
        "\n",
        "                # Đọc dữ liệu từng dòng\n",
        "                rows = []\n",
        "                skipped = 0\n",
        "                for line in data_content.split('\\n'):\n",
        "                    if line and not line.startswith('%'):\n",
        "                        try:\n",
        "                            # Tách dòng thành các phần\n",
        "                            parts = line.strip().split()\n",
        "                            if len(parts) >= 4:  # Kiểm tra có đủ 4 cột\n",
        "                                author1 = int(parts[0])\n",
        "                                author2 = int(parts[1])\n",
        "                                weight = float(parts[2])\n",
        "                                timestamp = int(parts[3])\n",
        "\n",
        "                                rows.append({\n",
        "                                    'author1': author1,\n",
        "                                    'author2': author2,\n",
        "                                    'weight': weight,\n",
        "                                    'timestamp': timestamp\n",
        "                                })\n",
        "                            else:\n",
        "                                skipped += 1\n",
        "                                print(f\"Dòng thiếu dữ liệu: {line}\")\n",
        "\n",
        "                        except (ValueError, IndexError) as e:\n",
        "                            skipped += 1\n",
        "                            print(f\"Lỗi xử lý dòng: {line} - {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                # Tạo DataFrame\n",
        "                df = pd.DataFrame(rows)\n",
        "\n",
        "                # In thông tin kiểm tra\n",
        "                print(f\"\\nKết quả đọc dữ liệu:\")\n",
        "                print(f\"- Số cạnh đã đọc thành công: {len(df):,}\")\n",
        "                print(f\"- Số dòng bị bỏ qua: {skipped:,}\")\n",
        "                print(\"\\nMẫu dữ liệu:\")\n",
        "                print(df.head())\n",
        "                print(\"\\nThông tin các cột:\")\n",
        "                print(df.dtypes)\n",
        "                print(\"\\nThống kê:\")\n",
        "                print(df.describe())\n",
        "\n",
        "                tar.close()\n",
        "                self.data = df\n",
        "                return True\n",
        "\n",
        "            else:\n",
        "                raise Exception(f\"Lỗi khi tải dữ liệu: Status code {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def create_graph(self):\n",
        "        \"\"\"Tạo đồ thị từ DataFrame\"\"\"\n",
        "        if self.data is None:\n",
        "            print(\"Chưa có dữ liệu để tạo đồ thị\")\n",
        "            return False\n",
        "\n",
        "        print(\"Đang tạo đồ thị...\")\n",
        "        try:\n",
        "            # Chỉ sử dụng các cột cần thiết cho đồ thị\n",
        "            self.graph = nx.from_pandas_edgelist(\n",
        "                self.data,\n",
        "                'author1',\n",
        "                'author2',\n",
        "                edge_attr=['weight', 'timestamp'],  # Thêm timestamp vào thuộc tính cạnh\n",
        "                create_using=nx.Graph()\n",
        "            )\n",
        "\n",
        "            print(f\"Đã tạo đồ thị với {self.graph.number_of_nodes():,} nodes và \"\n",
        "                  f\"{self.graph.number_of_edges():,} edges\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi khi tạo đồ thị: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_network(self):\n",
        "        \"\"\"Phân tích các đặc trưng của mạng\"\"\"\n",
        "        if self.graph is None:\n",
        "            print(\"Chưa có đồ thị để phân tích\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\nĐang phân tích mạng...\")\n",
        "\n",
        "        # 1. Thông tin cơ bản\n",
        "        start_time = time.time()\n",
        "        self.metrics['nodes'] = self.graph.number_of_nodes()\n",
        "        self.metrics['edges'] = self.graph.number_of_edges()\n",
        "        self.metrics['density'] = nx.density(self.graph)\n",
        "        print(f\"Thời gian tính thông tin cơ bản: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # 2. Degree metrics\n",
        "        start_time = time.time()\n",
        "        degrees = dict(self.graph.degree())\n",
        "        weighted_degrees = {\n",
        "            node: sum(dict(self.graph[node]).values())\n",
        "            for node in self.graph.nodes()\n",
        "        }\n",
        "\n",
        "        self.metrics['degrees'] = degrees\n",
        "        self.metrics['weighted_degrees'] = weighted_degrees\n",
        "        self.metrics['avg_degree'] = np.mean(list(degrees.values()))\n",
        "        self.metrics['max_degree'] = max(degrees.values())\n",
        "        self.metrics['avg_weighted_degree'] = np.mean(list(weighted_degrees.values()))\n",
        "        self.metrics['max_weighted_degree'] = max(weighted_degrees.values())\n",
        "        print(f\"Thời gian tính degree metrics: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # 3. Centrality measures\n",
        "        # Degree Centrality\n",
        "        start_time = time.time()\n",
        "        dc = nx.degree_centrality(self.graph)\n",
        "        self.metrics['degree_centrality'] = {\n",
        "            'values': dc,\n",
        "            'max': max(dc.values()),\n",
        "            'avg': np.mean(list(dc.values())),\n",
        "            'top': sorted(dc.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        }\n",
        "        print(f\"Thời gian tính Degree Centrality: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Betweenness Centrality\n",
        "        start_time = time.time()\n",
        "        bc = nx.betweenness_centrality(self.graph)\n",
        "        self.metrics['betweenness_centrality'] = {\n",
        "            'values': bc,\n",
        "            'max': max(bc.values()),\n",
        "            'avg': np.mean(list(bc.values())),\n",
        "            'top': sorted(bc.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        }\n",
        "        print(f\"Thời gian tính Betweenness Centrality: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Closeness Centrality\n",
        "        start_time = time.time()\n",
        "        cc = nx.closeness_centrality(self.graph)\n",
        "        self.metrics['closeness_centrality'] = {\n",
        "            'values': cc,\n",
        "            'max': max(cc.values()),\n",
        "            'avg': np.mean(list(cc.values())),\n",
        "            'top': sorted(cc.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        }\n",
        "        print(f\"Thời gian tính Closeness Centrality: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # PageRank\n",
        "        start_time = time.time()\n",
        "        pr = nx.pagerank(self.graph)\n",
        "        self.metrics['pagerank'] = {\n",
        "            'values': pr,\n",
        "            'max': max(pr.values()),\n",
        "            'avg': np.mean(list(pr.values())),\n",
        "            'top': sorted(pr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        }\n",
        "        print(f\"Thời gian tính PageRank: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def print_results(self):\n",
        "        \"\"\"In kết quả phân tích\"\"\"\n",
        "        if not self.metrics:\n",
        "            print(\"Chưa có kết quả phân tích\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nKẾT QUẢ PHÂN TÍCH MẠNG ĐỒNG TÁC GIẢ\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # 1. Thông tin cơ bản\n",
        "        print(\"\\n1. THÔNG TIN CƠ BẢN\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Số lượng tác giả: {self.metrics['nodes']:,}\")\n",
        "        print(f\"Số lượng cộng tác: {self.metrics['edges']:,}\")\n",
        "        print(f\"Mật độ mạng: {self.metrics['density']:.6f}\")\n",
        "        print(f\"Degree trung bình: {self.metrics['avg_degree']:.2f}\")\n",
        "        print(f\"Degree lớn nhất: {self.metrics['max_degree']}\")\n",
        "        print(f\"Weighted degree trung bình: {self.metrics['avg_weighted_degree']:.2f}\")\n",
        "        print(f\"Weighted degree lớn nhất: {self.metrics['max_weighted_degree']}\")\n",
        "\n",
        "        # 2. Centrality Measures\n",
        "        centrality_measures = {\n",
        "            'Degree Centrality': 'degree_centrality',\n",
        "            'Betweenness Centrality': 'betweenness_centrality',\n",
        "            'Closeness Centrality': 'closeness_centrality',\n",
        "            'PageRank': 'pagerank'\n",
        "        }\n",
        "\n",
        "        print(\"\\n2. CENTRALITY MEASURES\")\n",
        "        print(\"-\" * 30)\n",
        "        for name, measure in centrality_measures.items():\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"- Giá trị trung bình: {self.metrics[measure]['avg']:.6f}\")\n",
        "            print(f\"- Giá trị lớn nhất: {self.metrics[measure]['max']:.6f}\")\n",
        "            print(\"\\nTop 5 tác giả:\")\n",
        "            for author, score in self.metrics[measure]['top'][:5]:\n",
        "                print(f\"  {author}: {score:.6f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Khởi tạo analyzer\n",
        "    analyzer = NetworkAnalyzer()\n",
        "\n",
        "    # Tải và đọc dữ liệu\n",
        "    if not analyzer.download_and_read_data():\n",
        "        return\n",
        "\n",
        "    # Tạo đồ thị\n",
        "    if not analyzer.create_graph():\n",
        "        return\n",
        "\n",
        "     # Phân tích mạng\n",
        "    if not analyzer.analyze_network():\n",
        "        return\n",
        "\n",
        "    # In và trực quan hóa kết quả\n",
        "    analyzer.print_results()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yêu cầu\n",
        "## 1. Xây dựng phương thức: visualize_metrics trực quan hóa các số đo bằng biểu đồ\n",
        "Tạo figure với 2x2 subplots:\n",
        "- Degree Distribution\n",
        "- Centrality Comparison\n",
        "- Top nodes comparison\n",
        "....\n",
        "\n",
        "## 2. Từ các số đo hãy đưa ra nhận định cơ bản về mạng xã hội\n",
        "## 3. Thay đổi cách download dữ liệu trực tiếp thành việc đọc dữ liệu từ file:\n",
        "o\tFile được lưu trữ tại Google Drive (dành cho Google Colab)\n",
        "o\tFile được lưu trữ trực tiếp trên máy"
      ],
      "metadata": {
        "id": "cjIRLegkplwW"
      }
    }
  ]
}